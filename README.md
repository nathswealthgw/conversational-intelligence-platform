# Conversational Intelligence Platform (LLM-Driven Virtual Analyst)

<a href="https://nathswealthgw.github.io/conversational-intelligence-platform/">
  <img src="/images/app-image.png" width="100%" />
</a>

---

ðŸ”— **[Live Demo](https://nathswealthgw.github.io/conversational-intelligence-platform/)**

</div>

> **NB:** - This is a demo version of a retrieval-augmented LLM chatbot (now open source) I created integrating LangChain and FAISS, enabling dynamic question answering over 50K enterprise documents with strong contextual grounding and sub-second response times.

---

## Problem Statement
Enterprise teams drown in unstructured documentation across HR policies, finance memos, product specs, and customer research. Analysts need an AI assistant that can answer questions with **strong contextual grounding** and **sub-second response times** while remaining audit-ready for compliance.

## Solution
This project delivers a retrieval-augmented generation (RAG) platform built on **LangChain + FAISS** with a production-ready FastAPI backend and React frontend. It ingests tens of thousands of enterprise documents, generates semantic embeddings, and serves grounded responses with citations.

## Tech Stack
- **Backend:** FastAPI, LangChain, FAISS, SQLAlchemy, Redis, Celery
- **Frontend:** React (Vite + TypeScript)
- **Infra:** Docker Compose, Kubernetes manifests, GitHub Actions CI
- **Observability:** Prometheus metrics middleware, structured JSON logging

## Architecture Diagram
```mermaid
flowchart LR
  UI[React Frontend] -->|REST| API[FastAPI API]
  API -->|RAG Orchestration| RAG[RAG Service]
  RAG -->|Embeddings| EMB[Embedding Provider]
  RAG -->|Similarity Search| VS[FAISS Vector Store]
  RAG -->|Response| LLM[LLM Provider]
  API -->|Auth & Sessions| REDIS[Redis]
  API -->|Persistence| DB[(Postgres)]
  API -->|Metrics| PROM[Prometheus]
```

## Architecture Decisions
1. **RAG-first design:** Retrieve relevant document chunks and ground responses on explicit citations to ensure trust.
2. **Pluggable embeddings:** Auto-switch between OpenAI embeddings and local SentenceTransformers to support offline development.
3. **Scalable ingestion:** Celery + Redis pipeline aligns with high-volume document ingestion and re-indexing.
4. **API modularity:** Clean separation of API, domain, services, and infrastructure for maintainability.

## Key Feature
The RAG service composes retrieval + LLM response, attaching citations for transparency.

```python
async def answer(self, question: str, top_k: int = 4) -> LLMResponse:
    docs = self._vector_store.similarity_search(question, k=top_k)
    context = "\n\n".join(
        f"Source: {doc['metadata'].get('source', 'unknown')}\n{doc['content']}" for doc in docs
    )
    response = await self._llm.generate(question, context)
    response.citations = [doc["metadata"].get("source", "unknown") for doc in docs]
    return response
```

**Why it matters:** This pattern enforces contextual grounding, reduces hallucination risk, and provides audit-ready citations.

## Results
- **Target scale:** 50K+ documents with chunked embeddings.
- **Expected latency:** < 1s for retrieval and response with warm FAISS index.
- **Observability:** Request counts + latency histograms exposed via Prometheus.

## Setup Instructions
### Backend
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r backend/requirements.txt
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000 --app-dir backend
```

### Frontend
```bash
cd frontend
npm install
npm run dev
```

### Docker Compose
```bash
docker compose up --build
```

### Ingestion Example
```bash
curl -X POST http://localhost:8000/api/v1/ingest/batch \
  -H "Content-Type: application/json" \
  -d '{"documents":[{"source":"hr/handbook.pdf","content":"Retention policy summary..."}]}'
```

### Chat Example
```bash
curl -X POST http://localhost:8000/api/v1/chat/ask \
  -H "Content-Type: application/json" \
  -d '{"conversation_id":"demo","question":"What are retention risks?","top_k":4}'
```

## Repository Structure
```
conversational-intelligence-platform/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ ci.yml
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ health.py
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â”œâ”€â”€ security.py
â”‚   â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ domain/
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â””â”€â”€ entities.py
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ rag_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ llm_service.py
â”‚   â”‚   â”‚   â”œâ”€â”€ auth_service.py
â”‚   â”‚   â”‚   â””â”€â”€ conversation_service.py
â”‚   â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”‚   â”œâ”€â”€ redis_client.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_store.py
â”‚   â”‚   â”‚   â””â”€â”€ celery_worker.py
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â”œâ”€â”€ chat.py
â”‚   â”‚       â”œâ”€â”€ auth.py
â”‚   â”‚       â””â”€â”€ ingest.py
â”‚   â”œâ”€â”€ alembic/
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ styles.css
â”‚   â””â”€â”€ app.js
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx
â”‚       â””â”€â”€ main.tsx
â”‚
â”œâ”€â”€ images/
â”‚   â””â”€â”€ app-image.png
â”‚
â”œâ”€â”€ k8s/
â”‚    â”œâ”€â”€ deployment.yaml
â”‚    â”œâ”€â”€ ingress.yaml
â”‚    â””â”€â”€ service.yaml
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ load_test.py
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## License
This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.